{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Model Evaluation Demo.ipynb","version":"0.3.2","provenance":[{"file_id":"1KEEUh6hfyRmQ0HavPgeUo1dfr7H7yVVB","timestamp":1535314827133},{"file_id":"11KGMr4RSA-nnAdkqlze4ig3nEDivkviL","timestamp":1535313934601},{"file_id":"1dv4xMen2DtJhOUFKy9lyU3vgv4CbI6Jz","timestamp":1535154955897},{"file_id":"1XJKvIRxfo8G4K1m_m9JLoSbPYy1mDU_O","timestamp":1535153251477},{"file_id":"1MMcnnj9zTdi9kjuqKGCQ6fJ1T6T6BGSQ","timestamp":1521315848073}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"metadata":{"id":"1Z4qfrNxToV8","colab_type":"text"},"cell_type":"markdown","source":["**Note: Please make your own copy of this notebook to run and execute, thank you!**\n","\n","1.   Go to the menu tab on the top left corner\n","2.   Click on \"File\"\n","3.   Under the File tab menu click on \"Save a copy in Drive...\""]},{"metadata":{"id":"bWi4R-yKOUYE","colab_type":"text"},"cell_type":"markdown","source":["# Model Evaluation\n","\n","---"]},{"metadata":{"id":"W8op6xXujdJu","colab_type":"text"},"cell_type":"markdown","source":["### Data Science Pipeline for Predictive Machine Learning Models\n","\n","1. Import Libraries\n","\n","2. Load the Data\n","\n","3. Analyze the Dataset\n","\n","4. Clean, Transform, and Prepare Data\n","\n","5. Split the Data into Training and Testing Datasets\n","\n","6. Choose Performance Metric for Model\n","\n","7. Initialize our ML Model\n","\n","8. Train and Validate ML Algorithm with Multiple Parameters to Find Optimal Model\n","\n","9. Make Predictions"]},{"metadata":{"id":"5nGgxbIWQQy1","colab_type":"code","outputId":"4273af6b-ecce-40a9-bdcd-855000451c09","executionInfo":{"status":"ok","timestamp":1537467418719,"user_tz":420,"elapsed":317,"user":{"displayName":"Tairi Delgado","photoUrl":"//lh6.googleusercontent.com/-l4lv-T3P6-c/AAAAAAAAAAI/AAAAAAAABIA/4wtQ8ehIzZw/s50-c-k-no/photo.jpg","userId":"115188476170888991569"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"cell_type":"code","source":["# Toy example to load, analyze, clean, prepare, train and tune or model\n","# NOTE: In reality we should throughly inspect and analyze our data \n","\n","# Import libraries\n","import sklearn\n","import numpy as np\n","import pandas as pd\n","from sklearn.datasets import load_boston\n","from sklearn.tree import DecisionTreeRegressor\n","from sklearn.feature_selection import SelectKBest, f_classif\n","from sklearn.metrics import r2_score\n","from sklearn.metrics import make_scorer\n","from sklearn.model_selection import train_test_split\n","from sklearn.model_selection import ShuffleSplit\n","from sklearn.model_selection import StratifiedKFold\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.preprocessing import Normalizer\n","\n","# Load dataset\n","boston = load_boston()\n","features, prices = boston['data'], boston['target']\n","\n","# Descriptive Analysis of the data\n","features = pd.DataFrame(features)\n","features.describe()\n","\n","# Clean, transform, and prepare data (Not something we should automate - really should analze and understand dataset to make sure we have all the relevant data)\n","normal_features = Normalizer().fit_transform(features) # Normalize the features for easier processing\n","new_features = SelectKBest(f_classif, k=3).fit_transform(normal_features, prices) # Select the top 3 most relevant features (Should look into dimensionality reduction/feature engineering)\n","\n","# Split the data into training, test, features, and labels\n","features_train, features_test, prices_train, prices_test = train_test_split(new_features, prices, test_size=0.2, random_state=42)\n","\n","# Choose a performance metric (R2 evaluates how close the data matched the data)\n","scoring_fnc = make_scorer(r2_score)\n","\n","# Initialize our machine learning model (Decision Tree model that does regression)\n","regressor = DecisionTreeRegressor()\n","\n","# Train and validate best model\n","cv_sets = ShuffleSplit(prices_train.shape[0], test_size = 0.20, random_state = 0) # Takes our Training data and creates cross-validation sets\n","params = {'max_depth': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]} # Parameters for our model to tune (the depth of the decision tree to make a decision)\n","grid = GridSearchCV(regressor, param_grid = params, scoring = scoring_fnc) # Tune our model to find the right complexity\n","grid = grid.fit(features_train, prices_train) # Train our models with the training data\n","best_classifer = grid.best_estimator_ # Get the best model\n","\n","# Make new predictions (run several times and notice the test results are highly varied)\n","best_predictions = best_classifer.predict(features_test)\n","print(\"Final R2 score on the testing data: {:.4f}\".format(r2_score(prices_test, best_predictions)))\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Final R2 score on the testing data: 0.7913\n"],"name":"stdout"}]}]}